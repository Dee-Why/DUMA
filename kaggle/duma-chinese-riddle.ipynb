{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-23T21:37:42.365666Z","iopub.execute_input":"2023-04-23T21:37:42.366681Z","iopub.status.idle":"2023-04-23T21:37:42.372007Z","shell.execute_reply.started":"2023-04-23T21:37:42.366631Z","shell.execute_reply":"2023-04-23T21:37:42.370536Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\nfrom transformers import BertTokenizer, BertModel, BertConfig, BertForMultipleChoice\nfrom transformers import get_linear_schedule_with_warmup# , AdamW\nfrom transformers import logging\nfrom torch.nn import CrossEntropyLoss, MultiheadAttention\nfrom torch.optim import AdamW\nimport numpy as np\nimport datetime\nimport random\nimport json\nimport csv\nimport re\ndef separate_seq2(sequence_output, flat_input_ids):\n    qa_seq_output = sequence_output.new(sequence_output.size()).zero_()\n    qa_mask = torch.ones((sequence_output.shape[0], sequence_output.shape[1]),\n                         device=sequence_output.device,\n                         dtype=torch.bool)\n    p_seq_output = sequence_output.new(sequence_output.size()).zero_()\n    p_mask = torch.ones((sequence_output.shape[0], sequence_output.shape[1]),\n                        device=sequence_output.device,\n                        dtype=torch.bool)\n    for i in range(flat_input_ids.size(0)): # 这个是input\n        sep_lst = []\n        for idx, e in enumerate(flat_input_ids[i]):\n            if e == 2:\n                sep_lst.append(idx)\n        assert len(sep_lst) == 2\n        qa_seq_output[i, :sep_lst[0] - 1] = sequence_output[i, 1:sep_lst[0]]\n        qa_mask[i, :sep_lst[0] - 1] = 0\n        p_seq_output[i, :sep_lst[1] - sep_lst[0] -\n                     1] = sequence_output[i, sep_lst[0] + 1: sep_lst[1]]\n        p_mask[i, :sep_lst[1] - sep_lst[0] - 1] = 0\n    return qa_seq_output, p_seq_output, qa_mask, p_mask\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We \"pool\" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\nclass DUMALayer(nn.Module):\n    def __init__(self, d_model_size, num_heads):\n        super(DUMALayer, self).__init__()\n        self.attn_qa = MultiheadAttention(d_model_size, num_heads)\n        self.attn_p = MultiheadAttention(d_model_size, num_heads)\n\n    def forward(self, qa_seq_representation, p_seq_representation, qa_mask=None, p_mask=None):\n        qa_seq_representation = qa_seq_representation.permute([1, 0, 2]) # attention需要的输入尺寸为 sequence length, batch_size, d_model size\n        p_seq_representation = p_seq_representation.permute([1, 0, 2])\n        enc_output_qa, _ = self.attn_qa(\n            value=qa_seq_representation, key=qa_seq_representation, query=p_seq_representation, key_padding_mask=qa_mask\n        )\n        enc_output_p, _ = self.attn_p(\n            value=p_seq_representation, key=p_seq_representation, query=qa_seq_representation, key_padding_mask=p_mask\n        )\n        return enc_output_qa.permute([1, 0, 2]), enc_output_p.permute([1, 0, 2])\n\n\nclass DUMA(nn.Module):\n    def __init__(self, config, model_path, num_labels=5):\n        super(DUMA, self).__init__()\n        self.config = config\n        self.bert = BertModel.from_pretrained(model_path, config=self.config)\n        self.bert.gradient_checkpointing_enable()\n        self.duma = DUMALayer(d_model_size=self.config.hidden_size,\n                              num_heads=self.config.num_attention_heads)\n        self.pooler=BertPooler(config)\n        self.dropout = nn.Dropout(0.5) \n        self.classifier = nn.Linear(self.config.hidden_size, 1)\n        self.num_labels = num_labels\n\n    def forward(self, input_ids, token_type_ids, attention_mask):\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        last_output = outputs.last_hidden_state\n        qa_seq_output, p_seq_output, qa_mask, p_mask = separate_seq2(\n            last_output, input_ids)\n        enc_output_qa, enc_output_p = self.duma(\n            qa_seq_output, p_seq_output, qa_mask, p_mask)\n        fused_output = torch.cat([enc_output_qa, enc_output_p], dim=1)\n        \"\"\"\n        pooled_output = torch.mean(fused_output, dim=1)\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.classifier(dropout(pooled_output))\n            else:\n                logits += self.classifier(dropout(pooled_output))\n        logits = logits / len(self.dropouts)\n        \"\"\"\n        pooled_output=self.pooler(fused_output)\n        droped_output =self.dropout(pooled_output)\n        logits = self.classifier(droped_output)\n        # print(\"logits.shape:\",logits.shape, \"self.num_labels:\", self.num_labels) # torch[160,1]\n        reshaped_logits = F.softmax(logits.view(-1, self.num_labels), dim=1)\n        # print(reshaped_logits.shape) # torch.Size([32, 5])\n        # print(reshaped_logits) # 每一行的和为0，并且非负，是概率分布\n        return reshaped_logits\n\n\nclass DUMABert():\n    def __init__(self, train_path, validation_path, vocab_path, model_path, wiki_path,\n                 device, gpu, choices, max_len, train_batch_size, test_batch_size,\n                 learning_rate, epsilon, epoches, save_model_path,random_seed, config_path=None):\n\n        self.device = device\n        self.gpu = gpu\n        if self.gpu:\n            seed = random_seed\n            random.seed(seed)\n            np.random.seed(seed)\n            torch.manual_seed(seed)\n            torch.cuda.manual_seed_all(seed)\n            torch.backends.cudnn.deterministic = True\n        self.wiki_dicts = json.load(open(wiki_path, 'r', encoding='UTF-8'))\n        self.num_labels = choices\n        self.max_len = max_len\n        self.train_batch_size = train_batch_size\n        self.test_batch_size = test_batch_size\n        self.learning_rate = learning_rate\n        self.epsilon = epsilon\n        self.epoches = epoches\n        self.Tokenizer = BertTokenizer.from_pretrained(\n            model_path, cache_dir='./Albert', num_choices=5)\n        config = BertConfig.from_pretrained(model_path, num_choices=5)\n        self.config = config\n        # BertModel.from_pretrained(model_path,config=self.config)\n        self.model = DUMA(config=config, model_path=model_path)\n        self.train_data = self.load_data(train_path)\n        self.validation_data = self.load_data(validation_path, Test=True)\n        self.save_model_path = save_model_path\n\n    # 读取csv文件，从csv文件中加载文本数据\n    def read_file(self, path):\n        # 只能处理\n\n        text_list = []\n        labels = []\n        csv_file = open(path, encoding='UTF-8')\n        has_header = csv.Sniffer().has_header(csv_file.read(1024))\n        csv_file.seek(0)\n        file_lines = csv.reader(csv_file)  # 文件内容的每一行\n        if has_header:\n            next(file_lines)\n        for line in file_lines:\n            label = int(line[6])\n            text = str(line[0])\n            re_match=re.match(r'(.*)（(.*)）',text)\n            passage=re_match.group(1)\n            hint=str(re_match.group(2))+'?'\n            wiki_choices = []\n            for j in range(self.num_labels):\n                choice_text = str(line[j+1])  # 文本选项\n                choice_wiki = self.wiki_dicts[choice_text]  # 将选项定向到wiki文本解释\n                wiki_choices.append(hint+choice_text)\n            # 将问题重复num_labels次\n            content = [passage for i in range(self.num_labels)]\n            pairs = (content, wiki_choices)\n            text_list.append(pairs)\n            labels.append(label)\n        return text_list, labels\n\n\n\n    def encode_fn(self, text_list, labels):\n        input_ids, token_type_ids, attention_mask = [], [], []\n        for text in text_list:\n            encode_tokenizer = self.Tokenizer(text[1], text_pair=text[0], padding='max_length',\n                                              truncation=True,\n                                              max_length=self.max_len,\n                                              return_tensors='pt')  # 搞不懂这个text_pair有什么作用？\n            input_ids.append(encode_tokenizer['input_ids'].tolist())\n            token_type_ids.append(encode_tokenizer['token_type_ids'].tolist())\n            attention_mask.append(encode_tokenizer['attention_mask'].tolist())\n        labels = torch.tensor(labels)\n        input_ids = torch.tensor(input_ids)\n        token_type_ids = torch.tensor(token_type_ids)\n        attention_mask = torch.tensor(attention_mask)\n        # print(input_ids[3].data)#测试用\n        return TensorDataset(input_ids, token_type_ids, attention_mask, labels)\n    # 加载训练数据or测试数据\n\n    def load_data(self, path, Test=None):\n        text_list, labels = self.read_file(path)\n        Data = DataLoader(self.encode_fn(text_list, labels),\n                          batch_size=self.train_batch_size if not Test else self.test_batch_size,\n                          shuffle=False if Test else True,num_workers=2)  # 处理成多个batch的形式\n        return Data\n\n    def train_model(self):\n        if self.gpu:\n            self.model.to(self.device)\n        optimizer = AdamW(filter(lambda p: p.requires_grad, self.model.parameters(\n        )), lr=self.learning_rate, eps=self.epsilon)\n        #filter(lambda p: p.requires_grad, self.model.parameters())\n        epoches = self.epoches\n        trainData = self.train_data\n        testData = self.validation_data\n        total_steps = len(trainData) * epoches\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n        loss_Func = nn.CrossEntropyLoss()\n        t0 = datetime.datetime.now()\n        print('Train-----------')\n        print(f'Every epoch have {len(trainData)} steps.')\n        for epoch in range(epoches):\n            self.model.train()\n            train_loss = 0.0\n            test_loss = 0.0\n            test_accuracy = 0.0\n            train_accuracy = 0.0\n            print('Epoch: ', epoch+1)\n            for step, batch in enumerate(trainData):\n                self.model.zero_grad()\n                \n                # print(batch.shape)\n                \n                input_ids = batch[0].view(-1, batch[0].size(-1)) \n                attention_mask = batch[1].view(-1, batch[1].size(-1)) \n                token_type_ids = batch[2].view(-1, batch[2].size(-1)) \n                \n                labels = batch[3].to(self.device)\n                logits = self.model(input_ids=input_ids.to(self.device),\n                                    token_type_ids=attention_mask.to(self.device),\n                                    attention_mask=token_type_ids.to(self.device),\n                                    )\n\n                loss = loss_Func(logits, labels)\n                train_loss += loss.item()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(\n                    self.model.parameters(), 1.0)  # 避免过拟合？\n                optimizer.step()\n                scheduler.step()\n\n                logits = logits.detach()\n                every_train_accuracy = self.model_accuracy(logits, labels)\n                train_accuracy += every_train_accuracy\n                if step % 50 == 0 and step > 0:\n                    print('step:', step)\n                    print(f'Accuracy: {train_accuracy/(step+1):.4f}')\n            t1 = datetime.datetime.now()\n            print(f'Up to Epoch{epoch+1} Time: {t1-t0}')\n            avg_train_loss = train_loss/len(trainData)\n            print('Train loss: ', avg_train_loss)\n            print('Train acc: ', train_accuracy/len(trainData))\n\n            self.model.eval()\n            for k, test_batch in enumerate(testData):\n                with torch.no_grad():\n                    input_ids = test_batch[0].view(-1, test_batch[0].size(-1)) \n                    attention_mask = test_batch[1].view(-1, test_batch[1].size(-1)) \n                    token_type_ids = test_batch[2].view(-1, test_batch[1].size(-1)) \n                    labels = test_batch[3].to(self.device)\n                    logits = self.model(input_ids=input_ids.to(self.device),\n                                        token_type_ids=attention_mask.to(self.device),\n                                        attention_mask=token_type_ids.to(self.device),\n                                        )\n                    loss = loss_Func(logits, labels)\n                    test_loss += loss.item()\n                    logits = logits.detach()\n                    test_accuracy += self.model_accuracy(logits, labels)\n\n            avg_test_loss = test_loss/len(testData)\n            avg_test_acc = test_accuracy/len(testData)\n            print('Test--------------')\n            print('Test loss: ', avg_test_loss)\n            print('Test acc: ', avg_test_acc)\n            if epoch==0:\n                Epoch_avg_test_acc=avg_test_acc\n            \"\"\"\n            if avg_test_acc>0.5:\n                #保存模型\n                self.save_model(epoch)\n            \"\"\"\n\n        print('训练结束！')\n        t2 = datetime.datetime.now()\n        print(f'Total time: {t2-t0}')\n        return Epoch_avg_test_acc\n\n    def test_model(self):\n        if self.gpu:\n            self.model.to(self.device)\n        testData = self.validation_data\n        loss_Func = nn.CrossEntropyLoss()\n        test_loss = 0.0\n        test_accuracy = 0.0\n        self.model.eval()\n        for k, test_batch in enumerate(testData):\n            with torch.no_grad():\n                input_ids = test_batch[0].view(-1, test_batch[0].size(-1))\n                attention_mask = test_batch[1].view(-1, test_batch[1].size(-1))\n                token_type_ids = test_batch[2].view(-1, test_batch[1].size(-1))\n                labels = test_batch[3].to(self.device)\n                logits = self.model(input_ids=input_ids.to(self.device),\n                                    token_type_ids=attention_mask.to(self.device),\n                                    attention_mask=token_type_ids.to(self.device),\n                                    )\n                loss = loss_Func(logits, labels)\n                test_loss += loss.item()\n                logits = logits.detach()\n                test_accuracy += self.model_accuracy(logits, labels)\n\n        avg_test_loss = test_loss / len(testData)\n        avg_test_acc = test_accuracy / len(testData)\n        print('Test--------------')\n        print('Test loss: ', avg_test_loss)\n        print('Test acc: ', avg_test_acc)\n    \n    def save_model(self, index):\n        self.model.bert.save_pretrained(self.save_model_path+str(index)+'/'+'bert.sav')\n        self.Tokenizer.save_pretrained(self.save_model_path+str(index)+'/'+'tokenizer.sav')\n        torch.save(self.model, self.save_model_path+str(index)+'/'+'model.sav')\n        # model.save_pretrained(FIlE_PATH+'/Bert_Model/'+'-'+str(epoch))\n        # tokenizer.save_pretrained((FIlE_PATH+'/Bert_Model/'+'-'+str(epoch)))\n\n    def load_model(self, index):\n        self.model = torch.load(self.save_model_path + str(index) + '/' + 'model.sav')\n        self.model.bert = BertModel.from_pretrained(self.save_model_path + str(index) + '/' + 'bert.sav')\n        self.Tokenizer = BertTokenizer.from_pretrained(self.save_model_path + str(index) + '/' + 'tokenizer.sav')\n        \n    def val_model(self):\n        pass\n\n    def model_accuracy(self, logits, labels):\n        eq_logits = torch.eq(torch.max(logits, dim=1)[\n                             1], labels.flatten()).float()\n        acc = eq_logits.sum().item()/len(eq_logits)\n        return acc\n\n    def test_accuracy(self, logits, labels, input_ids, Error_File):\n        predict_labels = torch.max(logits, dim=1)[1]\n        acc_sum = 0.\n        for i in range(len(predict_labels)):\n            if predict_labels[i] == labels[i]:\n                acc_sum += 1.\n            else:\n                print(str(predict_labels[i])+'  '+str(\n                    self.Tokenizer.convert_ids_to_tokens(input_ids[i])+'\\n'), file=Error_File)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T21:37:42.400608Z","iopub.execute_input":"2023-04-23T21:37:42.400907Z","iopub.status.idle":"2023-04-23T21:37:42.461926Z","shell.execute_reply.started":"2023-04-23T21:37:42.400879Z","shell.execute_reply":"2023-04-23T21:37:42.460915Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    logging.set_verbosity_error()  # 只保留报错信息，而无warning信息\n    continue_training_on_index = None  # 在version 2 模型的基础上继续训练\n    save_index = 8 # 将本次训练的结果保存为version 8\n    epochs = 8 # 本次训练epoch数量\n    learning_rates = [3e-5,]\n    seed = [11,]\n    model_file = ''\n    trained_model_file = './Bert-base-trained_model'\n    model_name = '/Bert-RobertA/'\n    data_file = '/kaggle/input/chineseriddle/'\n\n    # Error_File_Name='./Error-Results.txt'\n    # Error_File=open(Error_File_Name,'w')\n    # 加载GPU\n    gpu = False\n    if torch.cuda.is_available():\n        gpu = True\n    device = torch.device(f'cuda:{1}' if torch.cuda.is_available() else 'cpu')\n\n    tri_avg_test_acc=0.0\n    bert_model = DUMABert(\n        train_path=data_file+'train.csv',\n        validation_path=data_file+'val.csv',\n        config_path=None,  # model_file+'/bert_config.json',\n        vocab_path='nghuyong/ernie-1.0',  # model_file+'/vocab.txt'\n        model_path='nghuyong/ernie-1.0',  # model_file+'/bert_model.bin'\n        wiki_path=data_file+'wiki_info_v3.json',\n        device=device,\n        gpu=gpu,\n        choices=5,\n        max_len=256,\n        train_batch_size=16,\n        test_batch_size=16,\n        learning_rate=learning_rates[0],\n        epsilon=1e-8,\n        epoches=epochs,\n        random_seed=seed[0],\n        save_model_path=trained_model_file+model_name,\n    )\n    \n    torch.cuda.empty_cache()\n    # 加载上一次训练的结果数据\n    if continue_training_on_index is not None:\n        print('loading model '+str(continue_training_on_index))\n        bert_model.load_model(continue_training_on_index)\n        bert_model.model.bert.gradient_checkpointing_enable()\n        \n        \n    # 训练过程\n    print('train_batch_size ',bert_model.train_batch_size)\n    print('learning_rate    ',bert_model.learning_rate)\n    print('seed ',seed[0])\n    tri_avg_test_acc+=bert_model.train_model()\n    print('tri_avg_test_acc ',np.mean(tri_avg_test_acc))\n    \n    # 保存训练结果\n    bert_model.save_model_path = './Bert-base-trained_model/Bert-RobertA/'\n    bert_model.model.bert.save_pretrained(bert_model.save_model_path+str(save_index)+'/'+'bert.sav')\n    bert_model.Tokenizer.save_pretrained(bert_model.save_model_path+str(save_index)+'/'+'tokenizer.sav')\n    torch.save(bert_model.model, bert_model.save_model_path+str(save_index)+'/'+'model.sav')","metadata":{"execution":{"iopub.status.busy":"2023-04-23T21:37:42.464351Z","iopub.execute_input":"2023-04-23T21:37:42.465189Z","iopub.status.idle":"2023-04-23T21:37:50.519356Z","shell.execute_reply.started":"2023-04-23T21:37:42.465149Z","shell.execute_reply":"2023-04-23T21:37:50.517377Z"},"trusted":true},"execution_count":42,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1549815388.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mepoches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msave_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrained_model_file\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/553709306.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_path, validation_path, vocab_path, model_path, wiki_path, device, gpu, choices, max_len, train_batch_size, test_batch_size, learning_rate, epsilon, epoches, save_model_path, random_seed, config_path)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# BertModel.from_pretrained(model_path,config=self.config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDUMA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_model_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/553709306.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, path, Test)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mtext_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         Data = DataLoader(self.encode_fn(text_list, labels),\n\u001b[0m\u001b[1;32m    200\u001b[0m                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTest\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                           shuffle=False if Test else True,num_workers=2)  # 处理成多个batch的形式\n","\u001b[0;32m/tmp/ipykernel_23/553709306.py\u001b[0m in \u001b[0;36mencode_fn\u001b[0;34m(self, text_list, labels)\u001b[0m\n\u001b[1;32m    183\u001b[0m                                               \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                                               \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                                               return_tensors='pt')  # 搞不懂这个text_pair有什么作用？\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_tokenizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_tokenizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2522\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2523\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2624\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2626\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2627\u001b[0m             )\n\u001b[1;32m   2628\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2815\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2816\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2817\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2818\u001b[0m         )\n\u001b[1;32m   2819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;31m# Simple mapping string => AddedToken for special tokens with specific tokenization behaviors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         all_special_tokens_extended = dict(\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens_extended\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAddedToken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mall_special_tokens_extended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[1;32m   1285\u001b[0m         \u001b[0mall_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m         \u001b[0mset_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens_map_extended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0mall_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_toks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mspecial_tokens_map_extended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0mset_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPECIAL_TOKENS_ATTRIBUTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m             \u001b[0mattr_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m                 \u001b[0mset_attr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# index = 1\n\n# loaded_model = DUMABert(\n#     train_path=data_file+'train.csv',\n#     validation_path=data_file+'val.csv',\n#     config_path=None,  # model_file+'/bert_config.json',\n#     vocab_path='nghuyong/ernie-1.0',  # model_file+'/vocab.txt'\n#     model_path='nghuyong/ernie-1.0',  # model_file+'/bert_model.bin'\n#     wiki_path=data_file+'wiki_info_v3.json',\n#     device=device,\n#     gpu=gpu,\n#     choices=5,\n#     max_len=256,\n#     train_batch_size=16,\n#     test_batch_size=16,\n#     learning_rate=learning_rates[0],\n#     epsilon=1e-8,\n#     epoches=epochs,\n#     random_seed=seed[0],\n#     save_model_path=trained_model_file+model_name,\n# )\n\n# loaded_model.load_model(1)\n\n# loaded_model.test_model()","metadata":{"execution":{"iopub.status.busy":"2023-04-23T21:37:50.520407Z","iopub.status.idle":"2023-04-23T21:37:50.521116Z","shell.execute_reply.started":"2023-04-23T21:37:50.520844Z","shell.execute_reply":"2023-04-23T21:37:50.520873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nDUMA Model\n'nghuyong/ernie-1.0'\nmax_len=256,\ntrain_batch_size=16,\ntest_batch_size=16,\nlearning_rate=6e-5,\nEvery epoch have 250 steps.\nEpoch:  1\nstep: 100\nAccuracy: 0.4802\nstep: 200\nAccuracy: 0.5177\nUp to Epoch1 Time: 0:10:36.416801\nTrain loss:  1.3710225167274475\nTrain acc:  0.538\nTest--------------\nTest loss:  1.3470525406301022\nTest acc:  0.556640625\nEpoch:  2\nstep: 100\nAccuracy: 0.6850\nstep: 200\nAccuracy: 0.6962\nUp to Epoch2 Time: 0:21:47.589869\nTrain loss:  1.2004402513504029\nTrain acc:  0.7015\nTest--------------\nTest loss:  1.3773974142968655\nTest acc:  0.525390625\n\n\nyou num_workers\nEvery epoch have 250 steps.\nEpoch:  1\nstep: 100\nAccuracy: 0.4802\nstep: 200\nAccuracy: 0.5177\nUp to Epoch1 Time: 0:10:36.416801\nTrain loss:  1.3710225167274475\nTrain acc:  0.538\nTest--------------\nTest loss:  1.3470525406301022\nTest acc:  0.556640625\n\nDUMA Model\n'nghuyong/ernie-1.0'\nmax_len=256,\ntrain_batch_size=16,\ntest_batch_size=16,\nlearning_rate=3e-5,\nEpoch:  3\nstep: 100\nAccuracy: 0.7351\nstep: 200\nAccuracy: 0.7441\nUp to Epoch3 Time: 0:41:24.260473\nTrain loss:  1.1650065503120421\nTrain acc:  0.74175\nTest--------------\nTest loss:  1.303547166287899\nTest acc:  0.59765625\n\nDUMA Model+pooler\n'nghuyong/ernie-1.0'\nmax_len=256,\ntrain_batch_size=16,\ntest_batch_size=16,\nlearning_rate=3e-5,\nEpoch:  1\nUp to Epoch1 Time: 0:12:57.225121\nTrain loss:  1.3917700653076173\nTrain acc:  0.522\nTest--------------\nTest loss:  1.3052873648703098\nTest acc:  0.615234375\nEpoch:  2\nUp to Epoch2 Time: 0:26:45.327771\nTrain loss:  1.2221286175251007\nTrain acc:  0.68625\nTest--------------\nTest loss:  1.2774858176708221\nTest acc:  0.625\n\nhint+?+baseline\nmax_len=256,\ntrain_batch_size=16,\ntest_batch_size=16,\nlearning_rate=8e-5,\nnohup: ignoring input\nTrain-----------\nEvery epoch have 125 steps.\nEpoch:  1\nstep: 100\nAccuracy: 0.5548\nUp to Epoch1 Time: 0:09:32.973775\nTrain loss:  1.075436095714569\nTrain acc:  0.57825\nTest--------------\nTest loss:  1.064383514225483\nTest acc:  0.603125\nEpoch:  2\nstep: 100\nAccuracy: 0.8020\nUp to Epoch2 Time: 0:22:04.232205\nTrain loss:  0.540188021659851\nTrain acc:  0.805\nTest--------------\nTest loss:  1.0543818064033985\nTest acc:  0.655859375\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-23T21:37:50.523498Z","iopub.status.idle":"2023-04-23T21:37:50.523989Z","shell.execute_reply.started":"2023-04-23T21:37:50.523732Z","shell.execute_reply":"2023-04-23T21:37:50.523757Z"},"trusted":true},"execution_count":null,"outputs":[]}]}