{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea96681d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T19:36:00.426820Z",
     "start_time": "2023-04-23T19:35:59.698439Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, BertForMultipleChoice\n",
    "from transformers import get_linear_schedule_with_warmup  # , AdamW\n",
    "from transformers import logging\n",
    "from torch.nn import CrossEntropyLoss, MultiheadAttention\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "def separate_seq2(sequence_output, flat_input_ids):\n",
    "    qa_seq_output = sequence_output.new(sequence_output.size()).zero_()\n",
    "    qa_mask = torch.ones((sequence_output.shape[0], sequence_output.shape[1]),\n",
    "                         device=sequence_output.device,\n",
    "                         dtype=torch.bool)\n",
    "    p_seq_output = sequence_output.new(sequence_output.size()).zero_()\n",
    "    p_mask = torch.ones((sequence_output.shape[0], sequence_output.shape[1]),\n",
    "                        device=sequence_output.device,\n",
    "                        dtype=torch.bool)\n",
    "    for i in range(flat_input_ids.size(0)):  # 这个是input\n",
    "        sep_lst = []\n",
    "        for idx, e in enumerate(flat_input_ids[i]):\n",
    "            if e == 2:\n",
    "                sep_lst.append(idx)\n",
    "        assert len(sep_lst) == 2\n",
    "        qa_seq_output[i, :sep_lst[0] - 1] = sequence_output[i, 1:sep_lst[0]]\n",
    "        qa_mask[i, :sep_lst[0] - 1] = 0\n",
    "        p_seq_output[i, :sep_lst[1] - sep_lst[0] -\n",
    "                         1] = sequence_output[i, sep_lst[0] + 1: sep_lst[1]]\n",
    "        p_mask[i, :sep_lst[1] - sep_lst[0] - 1] = 0\n",
    "    return qa_seq_output, p_seq_output, qa_mask, p_mask\n",
    "\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class DUMALayer(nn.Module):\n",
    "    def __init__(self, d_model_size, num_heads):\n",
    "        super(DUMALayer, self).__init__()\n",
    "        self.attn_qa = MultiheadAttention(d_model_size, num_heads)\n",
    "        self.attn_p = MultiheadAttention(d_model_size, num_heads)\n",
    "\n",
    "    def forward(self, qa_seq_representation, p_seq_representation, qa_mask=None, p_mask=None):\n",
    "        qa_seq_representation = qa_seq_representation.permute(\n",
    "            [1, 0, 2])  # attention需要的输入尺寸为 sequence length, batch_size, d_model size\n",
    "        p_seq_representation = p_seq_representation.permute([1, 0, 2])\n",
    "        enc_output_qa, _ = self.attn_qa(\n",
    "            value=qa_seq_representation, key=qa_seq_representation, query=p_seq_representation, key_padding_mask=qa_mask\n",
    "        )\n",
    "        enc_output_p, _ = self.attn_p(\n",
    "            value=p_seq_representation, key=p_seq_representation, query=qa_seq_representation, key_padding_mask=p_mask\n",
    "        )\n",
    "        return enc_output_qa.permute([1, 0, 2]), enc_output_p.permute([1, 0, 2])\n",
    "\n",
    "\n",
    "class DUMA(nn.Module):\n",
    "    def __init__(self, config, model_path, num_labels=5):\n",
    "        super(DUMA, self).__init__()\n",
    "        self.config = config\n",
    "        self.bert = BertModel.from_pretrained(model_path, config=self.config)\n",
    "        self.bert.gradient_checkpointing_enable()\n",
    "        self.duma = DUMALayer(d_model_size=self.config.hidden_size,\n",
    "                              num_heads=self.config.num_attention_heads)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, 1)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        last_output = outputs.last_hidden_state\n",
    "        qa_seq_output, p_seq_output, qa_mask, p_mask = separate_seq2(\n",
    "            last_output, input_ids)\n",
    "        enc_output_qa, enc_output_p = self.duma(\n",
    "            qa_seq_output, p_seq_output, qa_mask, p_mask)\n",
    "        fused_output = torch.cat([enc_output_qa, enc_output_p], dim=1)\n",
    "        \"\"\"\n",
    "        pooled_output = torch.mean(fused_output, dim=1)\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                logits = self.classifier(dropout(pooled_output))\n",
    "            else:\n",
    "                logits += self.classifier(dropout(pooled_output))\n",
    "        logits = logits / len(self.dropouts)\n",
    "        \"\"\"\n",
    "        pooled_output = self.pooler(fused_output)\n",
    "        droped_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(droped_output)\n",
    "        # print(\"logits.shape:\",logits.shape, \"self.num_labels:\", self.num_labels) # torch[160,1]\n",
    "        reshaped_logits = F.softmax(logits.view(-1, self.num_labels), dim=1)\n",
    "        # print(reshaped_logits.shape) # torch.Size([32, 5])\n",
    "        # print(reshaped_logits) # 每一行的和为0，并且非负，是概率分布\n",
    "        return reshaped_logits\n",
    "\n",
    "\n",
    "class DUMABert():\n",
    "    def __init__(self, train_path, validation_path, vocab_path, model_path, wiki_path,\n",
    "                 device, gpu, choices, max_len, train_batch_size, test_batch_size,\n",
    "                 learning_rate, epsilon, epoches, save_model_path, random_seed, config_path=None):\n",
    "\n",
    "        self.device = device\n",
    "        self.gpu = gpu\n",
    "        if self.gpu:\n",
    "            seed = random_seed\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "        self.wiki_dicts = json.load(open(wiki_path, 'r', encoding='UTF-8'))\n",
    "        self.num_labels = choices\n",
    "        self.max_len = max_len\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epoches = epoches\n",
    "        self.Tokenizer = BertTokenizer.from_pretrained(\n",
    "            model_path, cache_dir='./Albert', num_choices=5)\n",
    "        config = BertConfig.from_pretrained(model_path, num_choices=5)\n",
    "        self.config = config\n",
    "        # BertModel.from_pretrained(model_path,config=self.config)\n",
    "        self.model = DUMA(config=config, model_path=model_path)\n",
    "        self.train_data = self.load_data(train_path)\n",
    "        self.validation_data = self.load_data(validation_path, Test=True)\n",
    "        self.save_model_path = save_model_path\n",
    "\n",
    "    # 读取csv文件，从csv文件中加载文本数据\n",
    "    def read_file(self, path):\n",
    "        # 只能处理\n",
    "\n",
    "        text_list = []\n",
    "        labels = []\n",
    "        csv_file = open(path, encoding='UTF-8')\n",
    "        has_header = csv.Sniffer().has_header(csv_file.read(1024))\n",
    "        csv_file.seek(0)\n",
    "        file_lines = csv.reader(csv_file)  # 文件内容的每一行\n",
    "        if has_header:\n",
    "            next(file_lines)\n",
    "        for line in file_lines:\n",
    "            label = int(line[6])\n",
    "            text = str(line[0])\n",
    "            re_match = re.match(r'(.*)（(.*)）', text)\n",
    "            passage = re_match.group(1)\n",
    "            hint = str(re_match.group(2)) + '?'\n",
    "            wiki_choices = []\n",
    "            for j in range(self.num_labels):\n",
    "                choice_text = str(line[j + 1])  # 文本选项\n",
    "                choice_wiki = self.wiki_dicts[choice_text]  # 将选项定向到wiki文本解释\n",
    "                wiki_choices.append(hint + choice_text)\n",
    "            # 将问题重复num_labels次\n",
    "            content = [passage for i in range(self.num_labels)]\n",
    "            pairs = (content, wiki_choices)\n",
    "            text_list.append(pairs)\n",
    "            labels.append(label)\n",
    "        return text_list, labels\n",
    "\n",
    "    def encode_fn(self, text_list, labels):\n",
    "        input_ids, token_type_ids, attention_mask = [], [], []\n",
    "        for text in text_list:\n",
    "            encode_tokenizer = self.Tokenizer(text[1], text_pair=text[0], padding='max_length',\n",
    "                                              truncation=True,\n",
    "                                              max_length=self.max_len,\n",
    "                                              return_tensors='pt')  # 搞不懂这个text_pair有什么作用？\n",
    "            input_ids.append(encode_tokenizer['input_ids'].tolist())\n",
    "            token_type_ids.append(encode_tokenizer['token_type_ids'].tolist())\n",
    "            attention_mask.append(encode_tokenizer['attention_mask'].tolist())\n",
    "        labels = torch.tensor(labels)\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        token_type_ids = torch.tensor(token_type_ids)\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "        # print(input_ids[3].data)#测试用\n",
    "        return TensorDataset(input_ids, token_type_ids, attention_mask, labels)\n",
    "\n",
    "    # 加载训练数据or测试数据\n",
    "\n",
    "    def load_data(self, path, Test=None):\n",
    "        text_list, labels = self.read_file(path)\n",
    "        Data = DataLoader(self.encode_fn(text_list, labels),\n",
    "                          batch_size=self.train_batch_size if not Test else self.test_batch_size,\n",
    "                          shuffle=False if Test else True, num_workers=2)  # 处理成多个batch的形式\n",
    "        return Data\n",
    "\n",
    "    def train_model(self):\n",
    "        if self.gpu:\n",
    "            self.model.to(self.device)\n",
    "        optimizer = AdamW(filter(lambda p: p.requires_grad, self.model.parameters(\n",
    "        )), lr=self.learning_rate, eps=self.epsilon)\n",
    "        # filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        epoches = self.epoches\n",
    "        trainData = self.train_data\n",
    "        testData = self.validation_data\n",
    "        total_steps = len(trainData) * epoches\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "        loss_Func = nn.CrossEntropyLoss()\n",
    "        t0 = datetime.datetime.now()\n",
    "        print('Train-----------')\n",
    "        print(f'Every epoch have {len(trainData)} steps.')\n",
    "        for epoch in range(epoches):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            test_loss = 0.0\n",
    "            test_accuracy = 0.0\n",
    "            train_accuracy = 0.0\n",
    "            print('Epoch: ', epoch + 1)\n",
    "            for step, batch in enumerate(trainData):\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                # print(batch.shape)\n",
    "\n",
    "                input_ids = batch[0].view(-1, batch[0].size(-1))\n",
    "                attention_mask = batch[1].view(-1, batch[1].size(-1))\n",
    "                token_type_ids = batch[2].view(-1, batch[2].size(-1))\n",
    "\n",
    "                labels = batch[3].to(self.device)\n",
    "                logits = self.model(input_ids=input_ids.to(self.device),\n",
    "                                    token_type_ids=attention_mask.to(self.device),\n",
    "                                    attention_mask=token_type_ids.to(self.device),\n",
    "                                    )\n",
    "\n",
    "                loss = loss_Func(logits, labels)\n",
    "                train_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), 1.0)  # 避免过拟合？\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                logits = logits.detach()\n",
    "                every_train_accuracy = self.model_accuracy(logits, labels)\n",
    "                train_accuracy += every_train_accuracy\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    print('step:', step)\n",
    "                    print(f'Accuracy: {train_accuracy / (step + 1):.4f}')\n",
    "            t1 = datetime.datetime.now()\n",
    "            print(f'Up to Epoch{epoch + 1} Time: {t1 - t0}')\n",
    "            avg_train_loss = train_loss / len(trainData)\n",
    "            print('Train loss: ', avg_train_loss)\n",
    "            print('Train acc: ', train_accuracy / len(trainData))\n",
    "\n",
    "            self.model.eval()\n",
    "            for k, test_batch in enumerate(testData):\n",
    "                with torch.no_grad():\n",
    "                    input_ids = test_batch[0].view(-1, test_batch[0].size(-1))\n",
    "                    attention_mask = test_batch[1].view(-1, test_batch[1].size(-1))\n",
    "                    token_type_ids = test_batch[2].view(-1, test_batch[1].size(-1))\n",
    "                    labels = test_batch[3].to(self.device)\n",
    "                    logits = self.model(input_ids=input_ids.to(self.device),\n",
    "                                        token_type_ids=attention_mask.to(self.device),\n",
    "                                        attention_mask=token_type_ids.to(self.device),\n",
    "                                        )\n",
    "                    loss = loss_Func(logits, labels)\n",
    "                    test_loss += loss.item()\n",
    "                    logits = logits.detach()\n",
    "                    test_accuracy += self.model_accuracy(logits, labels)\n",
    "\n",
    "            avg_test_loss = test_loss / len(testData)\n",
    "            avg_test_acc = test_accuracy / len(testData)\n",
    "            print('Test--------------')\n",
    "            print('Test loss: ', avg_test_loss)\n",
    "            print('Test acc: ', avg_test_acc)\n",
    "            if epoch == 0:\n",
    "                Epoch_avg_test_acc = avg_test_acc\n",
    "            \"\"\"\n",
    "            if avg_test_acc>0.5:\n",
    "                #保存模型\n",
    "                self.save_model(epoch)\n",
    "            \"\"\"\n",
    "\n",
    "        print('训练结束！')\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f'Total time: {t2 - t0}')\n",
    "        return Epoch_avg_test_acc\n",
    "\n",
    "    def save_model(self, index):\n",
    "        self.model.bert.save_pretrained(self.save_model_path + str(index) + '/' + 'bert.sav')\n",
    "        self.Tokenizer.save_pretrained(self.save_model_path + str(index) + '/' + 'tokenizer.sav')\n",
    "        torch.save(self.model, self.save_model_path + str(index) + '/' + 'model.sav')\n",
    "        # model.save_pretrained(FIlE_PATH+'/Bert_Model/'+'-'+str(epoch))\n",
    "        # tokenizer.save_pretrained((FIlE_PATH+'/Bert_Model/'+'-'+str(epoch)))\n",
    "\n",
    "    def val_model(self):\n",
    "        pass\n",
    "\n",
    "    def model_accuracy(self, logits, labels):\n",
    "        eq_logits = torch.eq(torch.max(logits, dim=1)[\n",
    "                                 1], labels.flatten()).float()\n",
    "        acc = eq_logits.sum().item() / len(eq_logits)\n",
    "        return acc\n",
    "\n",
    "    def test_accuracy(self, logits, labels, input_ids, Error_File):\n",
    "        predict_labels = torch.max(logits, dim=1)[1]\n",
    "        acc_sum = 0.\n",
    "        for i in range(len(predict_labels)):\n",
    "            if predict_labels[i] == labels[i]:\n",
    "                acc_sum += 1.\n",
    "            else:\n",
    "                print(str(predict_labels[i]) + '  ' + str(\n",
    "                    self.Tokenizer.convert_ids_to_tokens(input_ids[i]) + '\\n'), file=Error_File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f65dba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T19:44:40.257700Z",
     "start_time": "2023-04-23T19:44:39.178196Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_bert = BertModel.from_pretrained('./Bert-base-trained_model/Bert-RobertA/1/bert.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "873afaca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T19:44:40.275077Z",
     "start_time": "2023-04-23T19:44:40.268491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(18000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(513, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): ReLU()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a86c4a8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T19:44:34.710618Z",
     "start_time": "2023-04-23T19:44:34.681236Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_tokenizer = BertTokenizer.from_pretrained('./Bert-base-trained_model/Bert-RobertA/1/tokenizer.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "707ba755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-23T19:44:35.449096Z",
     "start_time": "2023-04-23T19:44:35.443186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='./Bert-base-trained_model/Bert-RobertA/1/tokenizer.sav', vocab_size=17964, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac14bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
